{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepared by Vladimir Zaigrajew and Tymoteusz Kwieci≈Ñski\n",
    "\n",
    "# Exercise Week 5 - Interpretable Representation Learning\n",
    "In the previous exercise, we trained our first (maybe) self-supervised learning (SSL) model using rotation as a pretext task. Later, on a dataset of road signs, you tested how a fine-tuned SSL model performed comparably to a model trained from scratch with respect to the downstream task evaluation (road sign prediction). You could evaluate which supervised or self-supervised approach is better when we have a small amount of labeled data but a decent amount of unlabeled data. You also learned two evaluation methods in this task: using linear probing (freezing the model and training a classifier with features extracted from the SSL model) and fine-tuning on various downstream tasks.\n",
    "\n",
    "Next, you explored two other popular evaluation methods: using UMAP and t-SNE to map high-dimensional features to 2D space and visualize them, which enables visual understanding of how samples are distributed and how linear probing can be successful. The second approach is an alternative to linear probing, using a KNN classifier to evaluate the features extracted from the model. In the previous exercise, you evaluated models like a pro on several downstream tasks and datasets.\n",
    "\n",
    "Now for the last exercise, the cherry on the cake, this exercise is an ensemble of previous exercises where you will train your SSL model alongside a model from scratch and evaluate the features from both models on other downstream tasks with tools learned from previous exercises. However, now we are getting our hands on an SSL method which is well established in the SSL world and which you should already know from our lectures: [SimCLR](https://arxiv.org/abs/2002.05709). Your task will be to train two SSL models on the dataset with road signs alongside a model trained from scratch. We will be using an even smaller classification dataset than previously, and you will train models and compare them. To check everything we've done in previous exercises, we will also test how features from our models transfer to two other downstream tasks.\n",
    "\n",
    "So let's start!\n",
    "\n",
    "## Part I. Prepare the environment\n",
    "Firstly, prepare an environment by installing all the required libraries. To know which packages to install, you need to investigate cells with the import statements. In today's exercise, you will learn about a cool wrapper for PyTorch called `pytorch_lightning` which makes training and writing code easier :). I require you to paste the code below with the command to install the libraries with **specific versions**, for example:\n",
    "```python\n",
    "%pip install numpy==1.21.0 pandas==1.3.0 matplotlib==3.4.2 torch==1.9.0 torchvision==0.10.0 pytorch-lightning==1.4.9 scikit-learn==0.24.2 umap-learn==0.5.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import umap\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a task requiring you to implement the SimCLR model yourself would be interesting (or not because this one is fairly simple), we will leverage in this task another library called `lightly` which provides architectures and needed pipeline functions for various SSL models. In this exercise, we will adapt this tutorial from the [lightly library](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_simclr_clothing.html) to implement SimCLR in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.transforms import SimCLRTransform, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a very important part: making results reproducible by seeding the random number generators. In the code below, we set all the random number generators to the same seed. This is crucial for the reproducibility of results. You can change the seed to any number you want, but make sure to set it to the same number in all cells where random operations occur. For those who are still unfamiliar with this concept, understanding random seeding is essential knowledge for ML practitioners. Setting a fixed seed ensures that random operations (like weight initialization, data shuffling, etc.) produce the same results each time the code is run, which allows for consistent and comparable experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed everything for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed: int=42):\n",
    "    pl.seed_everything(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    ####### Normaly you would also need to seed those generators but `pytorch_lightning` does it in one func\n",
    "    # random.seed(seed)\n",
    "    # np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    ######\n",
    "    torch.cuda.manual_seed(seed) # Don't know if pytorch lightning does this\n",
    "    torch.cuda.manual_seed_all(seed) # Don't know if pytorch lightning does this\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Prepare the data\n",
    "\n",
    "Now, we need to prepare the data. In this exercise, similar to what was done two exercises ago, we will focus on the `GTSRB` dataset that consists of road sign images and their corresponding classes. The dataset contains 43 classes of traffic signs, with a total of approximately 50,000 images. The images are in color, and we will resize them to 224x224 pixels. The dataset is split into training and test sets, with about 39,209 images in the training set and 12,630 images in the test set. We will use **90%** of the training set for training our SSL models and the remaining **10%** for the classification task. The test set will be used for the evaluation of all models.\n",
    "\n",
    "To implement this, we will first download the dataset and create various preprocessed versions of it. We will create two versions of the `GTSRB` dataset with custom preprocessing required for our SSL models (consider why this specific preprocessing is needed). Additionally, we will also create training subsets and test datasets for the classification task. You don't need to implement anything here - just explore the code and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the transform to apply to the data\n",
    "# First the rotation transform\n",
    "rotation_transform = T.v2.Compose(\n",
    "    [\n",
    "        T.RandomResizedCrop((224, 224)),\n",
    "        T.v2.ToImage(),\n",
    "        T.v2.ToDtype(torch.float32, scale=True),\n",
    "        T.Normalize(\n",
    "            mean=utils.IMAGENET_NORMALIZE[\"mean\"],\n",
    "            std=utils.IMAGENET_NORMALIZE[\"std\"],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "# Now the transform for the SimCLR model\n",
    "simclr_transform = SimCLRTransform(input_size=(224, 224), vf_prob=0.5, rr_prob=0.5)\n",
    "# Now the transform for the classification model\n",
    "scratch_transform = T.v2.Compose(\n",
    "    [\n",
    "        T.RandomResizedCrop((224, 224)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.v2.ToImage(),\n",
    "        T.v2.ToDtype(torch.float32, scale=True),\n",
    "        T.Normalize(\n",
    "            mean=utils.IMAGENET_NORMALIZE[\"mean\"],\n",
    "            std=utils.IMAGENET_NORMALIZE[\"std\"],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "# Now the transform for the test dataset\n",
    "test_transform = T.v2.Compose(\n",
    "    [\n",
    "        T.Resize((224, 224)),\n",
    "        T.v2.ToImage(),\n",
    "        T.v2.ToDtype(torch.float32, scale=True),\n",
    "        T.Normalize(\n",
    "            mean=utils.IMAGENET_NORMALIZE[\"mean\"],\n",
    "            std=utils.IMAGENET_NORMALIZE[\"std\"],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the datasets\n",
    "path_where_data_is_stored = '../data' # change this to the path where you want to store the data\n",
    "\n",
    "# Load the GTSRB dataset\n",
    "train_rotation_dataset = torchvision.datasets.GTSRB(root=path_where_data_is_stored, split=\"train\", transform=rotation_transform, download=True)\n",
    "test_rotation_dataset = torchvision.datasets.GTSRB(root=path_where_data_is_stored, split=\"test\", transform=rotation_transform, download=True)\n",
    "\n",
    "train_simclr_dataset = torchvision.datasets.GTSRB(root=path_where_data_is_stored, split=\"train\", transform=simclr_transform, download=True)\n",
    "test_simclr_dataset = torchvision.datasets.GTSRB(root=path_where_data_is_stored, split=\"test\", transform=simclr_transform, download=True)\n",
    "\n",
    "train_dataset = torchvision.datasets.GTSRB(root=path_where_data_is_stored, split=\"train\", transform=scratch_transform, download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.GTSRB(root=path_where_data_is_stored, split=\"test\", transform=test_transform, download=True)\n",
    "\n",
    "# Lets now split the dataset into a SSL dataset and a classification dataset\n",
    "SSL_SIZE = 0.9 # percentage of the dataset to use for training\n",
    "targets = np.array([y for _, y in train_rotation_dataset])\n",
    "SSL_indices, classification_indices = train_test_split(\n",
    "    np.arange(len(targets)),\n",
    "    test_size=1-SSL_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=targets\n",
    ")\n",
    "train_rotation_dataset = torch.utils.data.Subset(train_rotation_dataset, SSL_indices)\n",
    "train_simclr_dataset = torch.utils.data.Subset(train_simclr_dataset, SSL_indices)\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, classification_indices)\n",
    "\n",
    "print(f\"Number of samples in the train dataset: {len(train_dataset)}\")\n",
    "print(f\"Number of samples in the test dataset: {len(test_dataset)}\")\n",
    "print(f\"Number of samples in the train rotation dataset: {len(train_rotation_dataset)}\")\n",
    "print(f\"Number of samples in the train simclr dataset: {len(train_simclr_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_idx_to_class = [\n",
    "    \"Speed limit (20km/h)\",\n",
    "    \"Speed limit (30km/h)\",\n",
    "    \"Speed limit (50km/h)\",\n",
    "    \"Speed limit (60km/h)\",\n",
    "    \"Speed limit (70km/h)\",\n",
    "    \"Speed limit (80km/h)\",\n",
    "    \"End of speed limit (80km/h)\",\n",
    "    \"Speed limit (100km/h)\",\n",
    "    \"Speed limit (120km/h)\",\n",
    "    \"No passing\",\n",
    "    \"No passing for vehicles over 3.5 metric tons\",\n",
    "    \"Right-of-way at the next intersection\",\n",
    "    \"Priority road\",\n",
    "    \"Yield\",\n",
    "    \"Stop\",\n",
    "    \"No vehicles\",\n",
    "    \"Vehicles over 3.5 metric tons prohibited\",\n",
    "    \"No entry\",\n",
    "    \"General caution\",\n",
    "    \"Dangerous curve to the left\",\n",
    "    \"Dangerous curve to the right\",\n",
    "    \"Double curve\",\n",
    "    \"Bumpy road\",\n",
    "    \"Slippery road\",\n",
    "    \"Road narrows on the right\",\n",
    "    \"Road work\",\n",
    "    \"Traffic signals\",\n",
    "    \"Pedestrians\",\n",
    "    \"Children crossing\",\n",
    "    \"Bicycles crossing\",\n",
    "    \"Beware of ice/snow\",\n",
    "    \"Wild animals crossing\",\n",
    "    \"End of all speed and passing limits\",\n",
    "    \"Turn right ahead\",\n",
    "    \"Turn left ahead\",\n",
    "    \"Ahead only\",\n",
    "    \"Go straight or right\",\n",
    "    \"Go straight or left\",\n",
    "    \"Keep right\",\n",
    "    \"Keep left\",\n",
    "    \"Roundabout mandatory\",\n",
    "    \"End of no passing\",\n",
    "    \"End of no passing by vehicles over 3.5 metric tons\"\n",
    "]\n",
    "angles = [0, 90, 180, 270]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have the original `GTSRB` dataset containing images and labels. We divided the training set into three parts: `train_rotation_dataset`, `train_simclr_dataset`, and `train_dataset` (classification dataset). The `train_dataset` will be used for the classification task, while the other two will be used for different SSL tasks. For SimCLR, we don't need to modify the dataset structure itself, as the `simclr_transform` already applies all needed modifications during training. However, for the rotation task, we need to prepare the dataset with random rotations. We've provided the class `SSLRot` which, unlike in the previous exercise where you needed to implement it yourself, you can now just relax and explore to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation Dataset\n",
    "class SSLRot(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset: torch.utils.data.Dataset, angles: list[int]):\n",
    "        super(SSLRot, self).__init__()\n",
    "        self.original_dataset = dataset\n",
    "        self.angles = angles\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset)\n",
    "    \n",
    "    def rand_rotate(self, img: torch.Tensor) -> tuple[torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Randomly rotates the image by 0, 90, 180, or 270 degrees.\n",
    "\n",
    "        Args:\n",
    "            img (torch.Tensor): Input image tensor of shape (C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            tuple: Rotated image tensor and the corresponding rotation label (0, 1, 2, or 3).\n",
    "        \"\"\"\n",
    "        rot_label = random.randint(0, 3) \n",
    "        rotated_img = T.functional.rotate(img, self.angles[rot_label])\n",
    "        return rotated_img, rot_label\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, int]:\n",
    "        # Get the data from the original dataset and ignore the label (second element)\n",
    "        img, _ = self.original_dataset[idx]\n",
    "        rotated_img, rot_label = self.rand_rotate(img)\n",
    "        return rotated_img, torch.tensor(rot_label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what the dataset looks like. We'll display some examples from our rotation-based SSL dataset. If you happen to get all 4 images rotated by 0 degrees, just rerun the cell. Even though we set a random seed for reproducibility, the specific rotation operations might not be entirely deterministic depending on implementation details, so you should see some variety in the rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rotation_dataset = SSLRot(train_rotation_dataset, angles)\n",
    "test_rotation_dataset = SSLRot(test_dataset, angles)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(6, 4))\n",
    "img, rot = train_rotation_dataset[0]\n",
    "img = img.numpy().transpose((1, 2, 0))\n",
    "img = utils.IMAGENET_NORMALIZE[\"std\"] * img + utils.IMAGENET_NORMALIZE[\"mean\"]\n",
    "img = np.clip(img, 0, 1)\n",
    "ax[0, 0].imshow(img)\n",
    "ax[0, 0].set_title(f\"Image rotation: {angles[rot]} degrees\")\n",
    "\n",
    "img, rot = train_rotation_dataset[1]\n",
    "img = img.numpy().transpose((1, 2, 0))\n",
    "img = utils.IMAGENET_NORMALIZE[\"std\"] * img + utils.IMAGENET_NORMALIZE[\"mean\"]\n",
    "img = np.clip(img, 0, 1)\n",
    "ax[0, 1].imshow(img)\n",
    "ax[0, 1].set_title(f\"Image rotation: {angles[rot]} degrees\")\n",
    "\n",
    "img, rot = test_rotation_dataset[2]\n",
    "img = img.numpy().transpose((1, 2, 0))\n",
    "img = utils.IMAGENET_NORMALIZE[\"std\"] * img + utils.IMAGENET_NORMALIZE[\"mean\"]\n",
    "img = np.clip(img, 0, 1)\n",
    "ax[1, 0].imshow(img)\n",
    "ax[1, 0].set_title(f\"Image rotation: {angles[rot]} degrees\")\n",
    "\n",
    "img, rot = test_rotation_dataset[3]\n",
    "img = img.numpy().transpose((1, 2, 0))\n",
    "img = utils.IMAGENET_NORMALIZE[\"std\"] * img + utils.IMAGENET_NORMALIZE[\"mean\"]\n",
    "img = np.clip(img, 0, 1)\n",
    "ax[1, 1].imshow(img)\n",
    "ax[1, 1].set_title(f\"Image rotation: {angles[rot]} degrees\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Load, modify and prepare training pipelines with ResNet18\n",
    "\n",
    "Now, we need to get the model. In the previous exercise, we used the `ResNet18` model from `torchvision`. This time we will use the same model, but we will modify it for different purposes: first to predict the rotation of the image as a self-supervised task, then to implement SimCLR, and finally to predict classes from the original dataset. The `ResNet18` model is a convolutional neural network (CNN) that is widely used for image classification tasks. It consists of 18 layers with residual connections (skip connections) that help address the vanishing gradient problem in deep networks, allowing it to learn rich representations from images efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resnet(number_of_classes: int | None = None) -> torch.nn.Module:\n",
    "    model = torchvision.models.resnet18(pretrained=False)\n",
    "    if number_of_classes is not None:\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, number_of_classes)\n",
    "    return model\n",
    "\n",
    "def test_load_resnet():\n",
    "    model = load_resnet(len(map_idx_to_class))\n",
    "    x, y = test_dataset[0]\n",
    "    x = x.unsqueeze(0)  # Add a batch dimension\n",
    "    y = torch.tensor(y).unsqueeze(0)  # Add a batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_y = model(x)\n",
    "        \n",
    "    loss = F.cross_entropy(pred_y, y)\n",
    "    pred_y_class = torch.argmax(pred_y, dim=1)\n",
    "    print(f\"Input shape: {x.shape}, Model output: {pred_y.shape}, Model predicted {pred_y_class}, Ground truth: {y}, Loss: {loss.item()}\")\n",
    "\n",
    "test_load_resnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your coding task begins. Based on the tutorial from [lightly](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_simclr_clothing.html), your task is to implement the `SimCLRModel` class using the PyTorch Lightning framework. This class will implement the SimCLR approach for self-supervised learning.\n",
    "\n",
    "The documentation and the class header will guide you through what needs to be implemented, but here are some hints:\n",
    "1. You'll need to use the ResNet18 backbone and modify it for the SimCLR approach\n",
    "2. Pay attention to how the projection head is implemented in SimCLR\n",
    "3. Follow the PyTorch Lightning structure with appropriate training_step, configure_optimizers, etc.\n",
    "4. The NT-Xent (normalized temperature-scaled cross entropy) loss function is key for SimCLR\n",
    "\n",
    "I believe you can implement this without too much additional help by following the tutorial from lightly and examining the class header. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRModel(pl.LightningModule):\n",
    "    def __init__(self, lr: float = 6e-2, weight_decay: float = 5e-4, max_epochs: int = 100):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're having difficulty implementing the SimCLR model, you can draw inspiration from the provided implementations of the rotation and classification Lightning models below. \n",
    "\n",
    "Importantly, there's a bug in how the training progress is logged in these implementations. Part of your task is to identify and fix this bug to ensure clear visualization of the training progress. Look carefully at how metrics are logged during training and validation steps, and check if there might be inconsistencies in naming, duplicate logging, or incorrect value calculations that could affect the training visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotationModel(pl.LightningModule):\n",
    "    def __init__(self, lr: float = 6e-2, weight_decay: float = 5e-4, max_epochs: int = 100, num_classes: int = 4):\n",
    "        super().__init__()\n",
    "        self.model = load_resnet(num_classes)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_epochs = max_epochs\n",
    "        self.save_hyperparameters(ignore=[\"model\"])\n",
    "        \n",
    "        # Add metrics for evaluation\n",
    "        self.metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.test_acc = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        \n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = self.metric(preds, y)    \n",
    "    \n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.train_acc.append(self.metric.compute())\n",
    "        self.log(\"train_acc_epoch\", self.metric.compute(), prog_bar=True)\n",
    "        self.metric.reset()\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        \n",
    "        # Calculate and log validation accuracy\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = self.metric(preds, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        self.val_acc.append(self.metric.compute())\n",
    "        self.log(\"val_acc_epoch\", self.metric.compute(), prog_bar=True)\n",
    "        self.metric.reset()\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        \n",
    "        # Calculate and log test accuracy\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = self.metric(preds, y)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        self.test_acc.append(self.metric.compute())\n",
    "        self.log(\"test_acc_epoch\", self.metric.compute(), prog_bar=True)\n",
    "        self.metric.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.AdamW(\n",
    "            self.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, self.max_epochs)\n",
    "        return [optim], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModel(pl.LightningModule):\n",
    "    def __init__(self,  model: torch.nn.Module, lr: float = 6e-2, weight_decay: float = 5e-4, max_epochs: int = 100):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_epochs = max_epochs\n",
    "        self.save_hyperparameters(ignore=[\"model\"])\n",
    "        \n",
    "        # Add metrics for evaluation\n",
    "        self.metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=len(map_idx_to_class))\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        self.test_acc = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        \n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = self.metric(preds, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.train_acc.append(self.metric.compute())\n",
    "        self.log(\"train_acc_epoch\", self.metric.compute(), prog_bar=True)\n",
    "        self.metric.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        \n",
    "        # Calculate and log validation accuracy\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = self.metric(preds, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        self.val_acc.append(self.metric.compute())\n",
    "        self.log(\"val_acc_epoch\", self.metric.compute(), prog_bar=True)\n",
    "        self.metric.reset()\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        \n",
    "\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = self.metric(preds, y)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        self.test_acc.append(self.metric.compute())\n",
    "        self.log(\"test_acc_epoch\", self.metric.compute(), prog_bar=True)\n",
    "        self.metric.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.AdamW(\n",
    "            self.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, self.max_epochs)\n",
    "        return [optim], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, here's our visualization method to display examples from the dataset alongside their ground truth labels and model predictions. This will help us visually evaluate how well our models are performing and understand any patterns in their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model: torch.nn.Module, dataset: torch.utils.data.Dataset, device: torch.device, class_names: list[str], num_images: int=5):\n",
    "    \"\"\"Visualize predictions of the model on a subset of the dataset.\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to use for predictions.\n",
    "        dataset (torch.utils.data.Dataset): The dataset to visualize.\n",
    "        device (torch.device): The device to use for predictions (CPU or GPU).\n",
    "        class_names (list[str]): The list of class names.\n",
    "        num_images (int): The number of images to visualize.\n",
    "        \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    indices = random.sample(range(len(dataset)), num_images)\n",
    "    \n",
    "    _, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            img, label = dataset[idx]\n",
    "            input = img.unsqueeze(0).to(device)\n",
    "            output = model(input)\n",
    "            pred_label = output.argmax(dim=1).item()\n",
    "            \n",
    "            img = img.numpy().transpose((1, 2, 0))\n",
    "            img = utils.IMAGENET_NORMALIZE[\"std\"] * img + utils.IMAGENET_NORMALIZE[\"mean\"]\n",
    "            img = np.clip(img, 0, 1)\n",
    "            \n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f\"Pred: {class_names[pred_label]}\\nTrue: {class_names[label]}\")\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV. Training SimCLR model\n",
    "\n",
    "Let's now train our models. First, we will start with training the SimCLR model `SimCLRModel` on our `train_simclr_dataset` using the PyTorch Lightning `Trainer` (I think you'll really appreciate its simplicity and power).\n",
    "\n",
    "PyTorch Lightning abstracts away much of the training loop boilerplate, letting you focus on the model architecture and logic. The `Trainer` handles device management, logging, checkpointing, and many other training details automatically.\n",
    "\n",
    "We encourage you to experiment with different parameters and see how they affect the training process. Increasing the batch_size and number of epochs can lead to better representations, but be mindful of memory usage and training time. Adjust these parameters based on your hardware capabilities and monitor the training performance. That's why we advise using Google Colab with GPU for this exercise.\n",
    "\n",
    "Some parameters you might want to experiment with:\n",
    "- Learning rate\n",
    "- Batch size\n",
    "- Number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS ###\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_WORKERS = 8\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data loaders\n",
    "dl_train_simclr = torch.utils.data.DataLoader(\n",
    "    train_simclr_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "dl_eval_simclr = torch.utils.data.DataLoader(\n",
    "    test_simclr_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "# Prepare the model\n",
    "simclr_model = SimCLRModel(lr=LEARNING_RATE, max_epochs=NUM_EPOCHS)\n",
    "\n",
    "# Prepare the trainer for pytorch lightning\n",
    "trainer = pl.Trainer(max_epochs=NUM_EPOCHS, devices=-1, accelerator=device)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(simclr_model, dl_train_simclr, dl_eval_simclr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now view the results of our model training by plotting the loss curves for both train and validation sets. This will help us understand how the model learning progressed and whether there are signs of overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = simclr_model.metric.plot(simclr_model.train_loss)\n",
    "fig.suptitle('Training Accuracy on Rotation Dataset')\n",
    "fig, ax = simclr_model.metric.plot(simclr_model.val_loss)\n",
    "fig.suptitle('Validation Accuracy on Rotation Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for later\n",
    "simclr_model = simclr_model.backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PyTorch Lightning, you can easily log the training progress and visualize it using TensorBoard. You can use the following command to start TensorBoard in your notebook:\n",
    "```python\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/\n",
    "```\n",
    "While we won't use TensorBoard in this exercise, it's a powerful tool for visualizing training metrics, model graphs, distributions of weights and biases, and much more. You're encouraged to explore it in your own projects.\n",
    "\n",
    "# Part V. Training Rotation model\n",
    "\n",
    "Let's now move on to train our rotation model `RotationModel` on the `train_rotation_dataset` with the same PyTorch Lightning trainer. The hyperparameters are the same as for the SimCLR model to maintain consistency in our comparison. You can also experiment with different parameters and see how they affect the training process.\n",
    "\n",
    "The training process is similar to the one we used for the SimCLR model, but we're using a different dataset (with rotated images) and a different model architecture tailored for the rotation prediction task. While both approaches are self-supervised, they learn representations in fundamentally different ways - SimCLR through contrastive learning of similar pairs, and the rotation model through predicting geometric transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS ###\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 12\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_WORKERS = 0\n",
    "NUMBER_OF_CLASSES = 4\n",
    "device = \"mps\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data loaders\n",
    "dl_train_rotation = torch.utils.data.DataLoader(\n",
    "    train_rotation_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "dl_eval_rotation = torch.utils.data.DataLoader(\n",
    "    test_rotation_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "# Prepare the model\n",
    "rotation_model = RotationModel(lr=LEARNING_RATE, max_epochs=NUM_EPOCHS, num_classes=NUMBER_OF_CLASSES)\n",
    "\n",
    "# Prepare the trainer for pytorch lightning\n",
    "trainer = pl.Trainer(max_epochs=NUM_EPOCHS, devices=-1, accelerator=device)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(rotation_model, dl_train_rotation, dl_eval_rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = rotation_model.metric.plot(rotation_model.train_acc)\n",
    "fig.suptitle('Training Accuracy on Rotation Dataset')\n",
    "fig, ax = rotation_model.metric.plot(rotation_model.val_acc)\n",
    "fig.suptitle('Validation Accuracy on Rotation Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_model = rotation_model.model\n",
    "rotation_model = rotation_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(rotation_model, test_rotation_dataset, num_images=5, device=device, class_names=angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VI. Training Classification model from scratch\n",
    "\n",
    "And the last model we are going to train is our classification model with random initialization (from scratch). We will train the `ClassificationModel` on the `train_dataset` with the same trainer. This model will serve as our baseline for comparison with the SSL-pretrained models we'll develop later.\n",
    "\n",
    "Training a model from scratch on the classification task will help us evaluate how effective our self-supervised pre-training methods are compared to the traditional supervised approach. We will also evaluate this model's performance on the test dataset to establish a baseline accuracy for our later comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS ###\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_WORKERS = 0\n",
    "NUMBER_OF_CLASSES = 43\n",
    "device = \"mps\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data loaders\n",
    "dl_train = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "dl_eval = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "# Prepare the model\n",
    "scratch_model = ClassifierModel(model=load_resnet(NUMBER_OF_CLASSES), lr=LEARNING_RATE, max_epochs=NUM_EPOCHS)\n",
    "\n",
    "# Prepare the trainer for pytorch lightning\n",
    "trainer = pl.Trainer(max_epochs=NUM_EPOCHS, devices=-1, accelerator=device)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(scratch_model, dl_train, dl_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = scratch_model.metric.plot(scratch_model.train_acc)\n",
    "fig.suptitle('Training and Validation Accuracy on Classification Dataset')\n",
    "fig, ax = scratch_model.metric.plot(scratch_model.val_acc)\n",
    "fig.suptitle('Training and Validation Accuracy on Classification Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_model = scratch_model.model\n",
    "scratch_model = scratch_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(scratch_model, test_dataset, num_images=5, device=device, class_names=map_idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VII. Let's evaluate the SSL models on the downstream task\n",
    "\n",
    "Now we will evaluate our SSL models on the downstream task of road sign image classification. We will use two common approaches to transfer learning from self-supervised models: linear probing and fine-tuning.\n",
    "\n",
    "## Linear Probing\n",
    "With linear probing, we will freeze the feature extraction backbone of our SSL models (preventing any weight updates) and train only a simple linear classifier on top of the extracted features. This approach tests how well the representations learned during self-supervised training transfer to the new task without any adaptation. Our `ClassifierModel` class already handles this by allowing us to freeze the backbone model before training.\n",
    "\n",
    "## Fine-Tuning\n",
    "With fine-tuning, we start with the same pre-trained model but allow the entire network (both backbone and classifier) to be updated during training. This approach allows the model to adapt its representations specifically for the downstream task. We'll use our `ClassificationModel` class for this approach as well, but without freezing the backbone.\n",
    "\n",
    "For both approaches, we will use the same trainer as before, but this time we will use the `train_dataset` for training and `test_dataset` for evaluation. This will allow us to directly compare:\n",
    "1. A model trained from scratch\n",
    "2. Linear probing of our rotation-based SSL model\n",
    "3. Linear probing of our SimCLR model\n",
    "4. Fine-tuning of our rotation-based SSL model\n",
    "5. Fine-tuning of our SimCLR model\n",
    "\n",
    "These comparisons will help us understand the effectiveness of different self-supervised learning approaches for representation learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VIII. Linear Probing on SimCLR model\n",
    "\n",
    "In this section, we'll apply linear probing to our pre-trained SimCLR model. This means we'll use the feature representations learned by SimCLR during self-supervised training, freeze those weights, and train only a linear classifier on top. This approach tests how linearly separable the learned features are for our road sign classification task without modifying the feature extractor itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS ###\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 21\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_WORKERS = 0\n",
    "NUMBER_OF_CLASSES = 43\n",
    "device = \"mps\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data loaders\n",
    "dl_train = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "dl_eval = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "# Prepare the base model\n",
    "model = deepcopy(simclr_model)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Add the linear layer to the Sequential model\n",
    "model = torch.nn.Sequential(\n",
    "    model,\n",
    "    torch.nn.Flatten(),  # Flatten the output of the pooling layer\n",
    "    torch.nn.Linear(512, NUMBER_OF_CLASSES)\n",
    ")\n",
    "\n",
    "# Prepare the model\n",
    "model = ClassifierModel(model=model, lr=LEARNING_RATE, max_epochs=NUM_EPOCHS)\n",
    "\n",
    "# Prepare the trainer for pytorch lightning\n",
    "trainer = pl.Trainer(max_epochs=NUM_EPOCHS, devices=-1, accelerator=device)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, dl_train, dl_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = model.metric.plot(model.train_acc)\n",
    "fig.suptitle('Training and Validation Accuracy with linear probing on simclr model on Classification Dataset')\n",
    "fig, ax = model.metric.plot(model.val_acc)\n",
    "fig.suptitle('Training and Validation Accuracy with linear probing on simclr model on Classification Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.model\n",
    "model = model.to(device)\n",
    "visualize_predictions(model, test_dataset, num_images=5, device=device, class_names=map_idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IX. Linear probing on rotation model\n",
    "\n",
    "Now we'll apply the same linear probing technique to our rotation-based self-supervised model. We'll freeze the feature extractor that was trained to predict image rotations and add a linear classifier on top for the road sign classification task. This will allow us to compare how well rotation prediction as a pretext task prepares the model for our downstream classification task, compared to the contrastive learning approach of SimCLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS ###\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 25\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_WORKERS = 0\n",
    "NUMBER_OF_CLASSES = 43\n",
    "device = \"mps\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data loaders\n",
    "dl_train = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "dl_eval = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "# Prepare the base model\n",
    "model = deepcopy(rotation_model)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, NUMBER_OF_CLASSES)\n",
    "\n",
    "# Prepare the model\n",
    "model = ClassifierModel(model=model, lr=LEARNING_RATE, max_epochs=NUM_EPOCHS)\n",
    "\n",
    "# Prepare the trainer for pytorch lightning\n",
    "trainer = pl.Trainer(max_epochs=NUM_EPOCHS, devices=-1, accelerator=device)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, dl_train, dl_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = model.metric.plot(model.train_acc)\n",
    "fig.suptitle('Training and Validation Accuracy with linear probing on rotation model on Classification Dataset')\n",
    "fig, ax = model.metric.plot(model.val_acc)\n",
    "fig.suptitle('Training and Validation Accuracy with linear probing on rotation model on Classification Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.model\n",
    "model = model.to(device)\n",
    "visualize_predictions(model, test_dataset, num_images=5, device=device, class_names=map_idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part X. Fine-tuning on SimCLR model\n",
    "\n",
    "In this section, we'll perform fine-tuning on our pre-trained SimCLR model. Unlike linear probing where we kept the feature extractor frozen, with fine-tuning we'll allow the entire network (both the SimCLR backbone and the classification head) to be updated during training. This approach allows the model to adapt its previously learned representations specifically for the road sign classification task. Fine-tuning typically provides better performance than linear probing but requires more computational resources and may be more prone to overfitting when the labeled dataset is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS ###\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_WORKERS = 0\n",
    "NUMBER_OF_CLASSES = 43\n",
    "device = \"mps\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data loaders\n",
    "dl_train = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "dl_eval = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "# Prepare the base model\n",
    "simclr_model_finetuned = deepcopy(simclr_model)\n",
    "\n",
    "# Add the linear layer to the Sequential model\n",
    "simclr_model_finetuned = torch.nn.Sequential(\n",
    "    simclr_model_finetuned,\n",
    "    torch.nn.Flatten(),  # Flatten the output of the pooling layer\n",
    "    torch.nn.Linear(512, NUMBER_OF_CLASSES)\n",
    ")\n",
    "\n",
    "# Prepare the model\n",
    "simclr_model_finetuned = ClassifierModel(model=simclr_model_finetuned, lr=LEARNING_RATE, max_epochs=NUM_EPOCHS)\n",
    "\n",
    "# Prepare the trainer for pytorch lightning\n",
    "trainer = pl.Trainer(max_epochs=NUM_EPOCHS, devices=-1, accelerator=device)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(simclr_model_finetuned, dl_train, dl_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = simclr_model_finetuned.metric.plot(simclr_model_finetuned.train_acc)\n",
    "fig.suptitle('Training and Validation Accuracy with fine-tuning on simclr model on Classification Dataset')\n",
    "fig, ax = simclr_model_finetuned.metric.plot(simclr_model_finetuned.val_acc)\n",
    "fig.suptitle('Training and Validation Accuracy with fine-tuning on simclr model on Scratch Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr_model_finetuned = simclr_model_finetuned.model\n",
    "simclr_model_finetuned = simclr_model_finetuned.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(simclr_model_finetuned, test_dataset, num_images=5, device=device, class_names=map_idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part XI. Fine-tuning on rotation model\n",
    "\n",
    "Now we'll apply fine-tuning to our rotation-based self-supervised model. We'll take the model that was pre-trained to predict image rotations and allow all its parameters to be updated while training on the road sign classification task. This will enable us to compare the effectiveness of fine-tuning versus linear probing for the rotation-based model, as well as compare the performance of fine-tuned rotation-based representations against fine-tuned SimCLR representations. This comparison will provide insights into which self-supervised learning approach provides better transferable features for our specific downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS ###\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_WORKERS = 0\n",
    "NUMBER_OF_CLASSES = 43\n",
    "device = \"mps\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data loaders\n",
    "dl_train = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "dl_eval = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "# Prepare the base model\n",
    "rotation_model_model_finetuned = deepcopy(rotation_model)\n",
    "rotation_model_model_finetuned.fc = torch.nn.Linear(rotation_model_model_finetuned.fc.in_features, NUMBER_OF_CLASSES)\n",
    "\n",
    "# Prepare the model\n",
    "rotation_model_model_finetuned = ClassifierModel(model=rotation_model_model_finetuned, lr=LEARNING_RATE, max_epochs=NUM_EPOCHS)\n",
    "\n",
    "# Prepare the trainer for pytorch lightning\n",
    "trainer = pl.Trainer(max_epochs=NUM_EPOCHS, devices=-1, accelerator=device)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(rotation_model_model_finetuned, dl_train, dl_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = rotation_model_model_finetuned.metric.plot(rotation_model_model_finetuned.train_acc)\n",
    "fig.suptitle('Training and Validation Accuracy with fine-tuning on rotation model on Classification Dataset')\n",
    "fig, ax = rotation_model_model_finetuned.metric.plot(rotation_model_model_finetuned.val_acc)\n",
    "fig.suptitle('Training and Validation Accuracy with fine-tuning on rotation model on Classification Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_model_model_finetuned = rotation_model_model_finetuned.model\n",
    "rotation_model_model_finetuned = rotation_model_model_finetuned.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(rotation_model_model_finetuned, test_dataset, num_images=5, device=device, class_names=map_idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part XII. PCA visualization\n",
    "\n",
    "The last model that we are going to incorporate is our old friend, the `PCA` (Principal Component Analysis) model. PCA is a classical unsupervised learning method used for dimensionality reduction. I won't go into the mathematical details of PCA as you should already know how it works from previous courses. \n",
    "\n",
    "In the cell below, you need to implement PCA on the GTSRB train subset. The requirements are:\n",
    "1. Apply PCA to **flattened images** (convert each image to a 1D array)\n",
    "2. Set the number of components equal to the dimension of embeddings we're using from our ResNet models (`scratch_model.fc.in_features`)\n",
    "3. Ensure that the data is standardized before fitting PCA (use `Pipeline` from the `sklearn` library to combine the standardization step with PCA)\n",
    "\n",
    "This will allow us to compare a classical dimensionality reduction technique with our deep learning-based approaches. PCA will serve as another baseline to understand how well simple linear methods capture the structure in our data compared to the more complex self-supervised learning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dataset = torchvision.datasets.GTSRB(root=path_where_data_is_stored, split=\"train\", transform=test_transform, download=True)\n",
    "indices = random.sample(range(len(pca_dataset)), 5000)\n",
    "# Placeholder for the PCA model you need to create and train\n",
    "\n",
    "# Visualize the explained variance ratio of the PCA components\n",
    "plt.plot(np.cumsum(pca.named_steps['pca'].explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part XIII. Visualization of the features\n",
    "\n",
    "Below is the code that will visualize the features extracted from our models. The visualization is done using UMAP and t-SNE methods, both of which reduce the dimensionality of the features to 2 dimensions for easy visualization. \n",
    "\n",
    "UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-distributed Stochastic Neighbor Embedding) are widely used techniques for visualizing high-dimensional data in a lower-dimensional space:\n",
    "- UMAP is based on the concept of topological data analysis and tends to preserve both local and global structure\n",
    "- t-SNE is based on probabilistic modeling and focuses more on preserving local neighborhood structure\n",
    "\n",
    "These visualizations will help us understand the quality of the learned representations from each model. Good representations should show clear clustering of samples from the same class, indicating that the model has learned features that separate different classes well.\n",
    "\n",
    "When examining these visualizations, look for:\n",
    "1. Clear separation between different classes (indicated by colors)\n",
    "2. Tight clustering of samples from the same class\n",
    "3. Meaningful structure in the overall distribution\n",
    "4. Differences between representations from different models (SimCLR vs. Rotation vs. Scratch vs. PCA)\n",
    "\n",
    "The code below is already prepared for you, and you don't need to modify it. You can just run it and see the results. The code will visualize the features extracted from the models on our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "def plot_tsne(\n",
    "    simclr_features: np.ndarray,\n",
    "    rotation_features: np.ndarray,\n",
    "    scratch_features: np.ndarray,\n",
    "    simclr_model_finetuned_features: np.ndarray,\n",
    "    rotation_model_model_finetuned_features: np.ndarray,\n",
    "    pca_features: np.ndarray,\n",
    "    labels: np.ndarray\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the features using t-SNE with a color gradient bar at the bottom.\n",
    "\n",
    "    Args:\n",
    "        simclr_features (np.ndarray): Features from the SimCLR model.\n",
    "        rotation_features (np.ndarray): Features from the rotation model.\n",
    "        scratch_features (np.ndarray): Features from the scratch model.\n",
    "        simclr_model_finetuned_features (np.ndarray): Features from the fine-tuned SimCLR model.\n",
    "        rotation_model_model_finetuned_features (np.ndarray): Features from the fine-tuned rotation model.\n",
    "        pca_features (np.ndarray): Features from the PCA model.\n",
    "        labels (np.ndarray): Labels for the data points.\n",
    "    \"\"\"\n",
    "    # Get TSNE\n",
    "    scaler = StandardScaler()\n",
    "    scaler_simclr_features = scaler.fit_transform(simclr_features)\n",
    "    scaler_rotation_features = scaler.fit_transform(rotation_features)\n",
    "    scaler_scratch_features = scaler.fit_transform(scratch_features)\n",
    "    scaler_simclr_model_finetuned_features = scaler.fit_transform(simclr_model_finetuned_features)\n",
    "    scaler_rotation_model_model_finetuned_features = scaler.fit_transform(rotation_model_model_finetuned_features)\n",
    "    scaler_pca_features = scaler.fit_transform(pca_features)\n",
    "    \n",
    "    # Apply TSNE\n",
    "    tsne = TSNE(n_components=2, random_state=SEED)\n",
    "    tsne_simclr_features = tsne.fit_transform(scaler_simclr_features)\n",
    "    tsne_rotation_features = tsne.fit_transform(scaler_rotation_features)\n",
    "    tsne_scratch_features = tsne.fit_transform(scaler_scratch_features)\n",
    "    tsne_simclr_model_finetuned_features = tsne.fit_transform(scaler_simclr_model_finetuned_features)\n",
    "    tsne_rotation_model_model_finetuned_features = tsne.fit_transform(scaler_rotation_model_model_finetuned_features)\n",
    "    tsne_pca_features = tsne.fit_transform(scaler_pca_features)\n",
    "    \n",
    "    # Create a figure with GridSpec for better control\n",
    "    fig = plt.figure(figsize=(20, 14))\n",
    "    gs = gridspec.GridSpec(3, 3, height_ratios=[1, 1, 0.15])\n",
    "    \n",
    "    # Get unique classes for colormap\n",
    "    unique_labels = np.unique(labels)\n",
    "    num_classes = len(unique_labels)\n",
    "    \n",
    "    # Create a color map\n",
    "    cmap = plt.cm.get_cmap('viridis', num_classes)\n",
    "    colors = [cmap(i) for i in range(num_classes)]\n",
    "    \n",
    "    # Plot 1: Model Trained from Scratch\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax1.scatter(\n",
    "            tsne_scratch_features[mask, 0], \n",
    "            tsne_scratch_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7,\n",
    "            label=str(label)  # Convert label to string for plotting\n",
    "        )\n",
    "    ax1.set_title('Model Trained from Scratch', fontsize=16)\n",
    "    ax1.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax1.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Plot 2: Model Trained with SimCLR\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax2.scatter(\n",
    "            tsne_simclr_features[mask, 0], \n",
    "            tsne_simclr_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax2.set_title('Model Trained with SimCLR', fontsize=16)\n",
    "    ax2.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax2.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Plot 3: Model Trained with Rotation\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax3.scatter(\n",
    "            tsne_rotation_features[mask, 0], \n",
    "            tsne_rotation_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax3.set_title('Model Trained with Rotation', fontsize=16)\n",
    "    ax3.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax3.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Plot 4: Fine-tuned SimCLR Model\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax4.scatter(\n",
    "            tsne_simclr_model_finetuned_features[mask, 0], \n",
    "            tsne_simclr_model_finetuned_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax4.set_title('Fine-tuned SimCLR Model', fontsize=16)\n",
    "    ax4.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax4.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Plot 5: Fine-tuned Rotation Model\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax5.scatter(\n",
    "            tsne_rotation_model_model_finetuned_features[mask, 0], \n",
    "            tsne_rotation_model_model_finetuned_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax5.set_title('Fine-tuned Rotation Model', fontsize=16)\n",
    "    ax5.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax5.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Plot 6: PCA\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax6.scatter(\n",
    "            tsne_pca_features[mask, 0], \n",
    "            tsne_pca_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax6.set_title('PCA', fontsize=16)\n",
    "    ax6.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax6.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Create a horizontal colorbar at the bottom\n",
    "    cbar_ax = fig.add_subplot(gs[2, :])\n",
    "    norm = Normalize(vmin=0, vmax=num_classes-1)\n",
    "    cb = ColorbarBase(\n",
    "        cbar_ax, \n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        orientation='horizontal'\n",
    "    )\n",
    "    \n",
    "    # Set colorbar title\n",
    "    cb.set_label(f'Classes (Total: {num_classes})', fontsize=14)\n",
    "    \n",
    "    # Add ticks for each class on the colorbar\n",
    "    # For a large number of classes, we'll space them out\n",
    "    if num_classes <= 20:\n",
    "        # For a smaller number of classes, show all ticks\n",
    "        tick_positions = np.arange(num_classes)\n",
    "        tick_labels = [str(label) for label in unique_labels]\n",
    "    else:\n",
    "        # For many classes, show every nth class\n",
    "        n = max(1, num_classes // 20)  # Show at most 20 ticks\n",
    "        tick_positions = np.arange(0, num_classes, n)\n",
    "        tick_labels = [str(unique_labels[i]) for i in range(0, num_classes, n)]\n",
    "    \n",
    "    cb.set_ticks(tick_positions)\n",
    "    cb.set_ticklabels(tick_labels)\n",
    "    \n",
    "    # Calculate frequency of each class\n",
    "    class_counts = {}\n",
    "    for label in labels:\n",
    "        if label in class_counts:\n",
    "            class_counts[label] += 1\n",
    "        else:\n",
    "            class_counts[label] = 1\n",
    "    \n",
    "    # Add frequency information above the colorbar\n",
    "    if num_classes <= 40:\n",
    "        # Add text annotations for class frequencies\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            if i % (max(1, num_classes // 20)) == 0:  # Only show labels for ticks we're displaying\n",
    "                freq = class_counts.get(label, 0)\n",
    "                cbar_ax.text(\n",
    "                    i/(num_classes-1), \n",
    "                    1.1, \n",
    "                    f\"{freq}\", \n",
    "                    horizontalalignment='center',\n",
    "                    fontsize=8\n",
    "                )\n",
    "    \n",
    "    # Set the main title\n",
    "    plt.suptitle(f'Comparing Feature Representations Using t-SNE Visualization\\n(Total Classes: {num_classes})', fontsize=20)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_umap(\n",
    "    simclr_features: np.ndarray,\n",
    "    rotation_features: np.ndarray,\n",
    "    scratch_features: np.ndarray,\n",
    "    simclr_model_finetuned_features: np.ndarray,\n",
    "    rotation_model_model_finetuned_features: np.ndarray,\n",
    "    pca_features: np.ndarray,\n",
    "    labels: np.ndarray\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the features using UMAP with a color gradient bar at the bottom.\n",
    "\n",
    "    Args:\n",
    "        simclr_features (np.ndarray): Features from the SimCLR model.\n",
    "        rotation_features (np.ndarray): Features from the rotation model.\n",
    "        scratch_features (np.ndarray): Features from the scratch model.\n",
    "        simclr_model_finetuned_features (np.ndarray): Features from the fine-tuned SimCLR model.\n",
    "        rotation_model_model_finetuned_features (np.ndarray): Features from the fine-tuned rotation model.\n",
    "        pca_features (np.ndarray): Features from the PCA model.\n",
    "        labels (np.ndarray): Labels for the data points.\n",
    "    \"\"\"\n",
    "    # Get UMAP\n",
    "    scaler = StandardScaler()\n",
    "    scaler_simclr_features = scaler.fit_transform(simclr_features)\n",
    "    scaler_rotation_features = scaler.fit_transform(rotation_features)\n",
    "    scaler_scratch_features = scaler.fit_transform(scratch_features)\n",
    "    scaler_simclr_model_finetuned_features = scaler.fit_transform(simclr_model_finetuned_features)\n",
    "    scaler_rotation_model_model_finetuned_features = scaler.fit_transform(rotation_model_model_finetuned_features)\n",
    "    scaler_pca_features = scaler.fit_transform(pca_features)\n",
    "    \n",
    "    # Apply UMAP\n",
    "    umap_model = umap.UMAP(n_components=2, random_state=SEED)\n",
    "    umap_simclr_features = umap_model.fit_transform(scaler_simclr_features)\n",
    "    umap_rotation_features = umap_model.fit_transform(scaler_rotation_features)\n",
    "    umap_scratch_features = umap_model.fit_transform(scaler_scratch_features)\n",
    "    umap_simclr_model_finetuned_features = umap_model.fit_transform(scaler_simclr_model_finetuned_features)\n",
    "    umap_rotation_model_model_finetuned_features = umap_model.fit_transform(scaler_rotation_model_model_finetuned_features)\n",
    "    umap_pca_features = umap_model.fit_transform(scaler_pca_features)\n",
    "    \n",
    "    # Create a figure with GridSpec for better control\n",
    "    fig = plt.figure(figsize=(20, 14))\n",
    "    gs = gridspec.GridSpec(3, 3, height_ratios=[1, 1, 0.15])\n",
    "    \n",
    "    # Get unique classes for colormap\n",
    "    unique_labels = np.unique(labels)\n",
    "    num_classes = len(unique_labels)\n",
    "    \n",
    "    # Create a color map\n",
    "    cmap = plt.cm.get_cmap('viridis', num_classes)\n",
    "    colors = [cmap(i) for i in range(num_classes)]\n",
    "    \n",
    "    # Plot 1: Model Trained from Scratch\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax1.scatter(\n",
    "            umap_scratch_features[mask, 0], \n",
    "            umap_scratch_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7,\n",
    "            label=str(label)  # Convert label to string for plotting\n",
    "        )\n",
    "    ax1.set_title('Model Trained from Scratch', fontsize=16)\n",
    "    ax1.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax1.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Plot 2: Model Trained with SimCLR\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax2.scatter(\n",
    "            umap_simclr_features[mask, 0], \n",
    "            umap_simclr_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax2.set_title('Model Trained with SimCLR', fontsize=16)\n",
    "    ax2.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax2.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Plot 3: Model Trained with Rotation\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax3.scatter(\n",
    "            umap_rotation_features[mask, 0], \n",
    "            umap_rotation_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax3.set_title('Model Trained with Rotation', fontsize=16)\n",
    "    ax3.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax3.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Plot 4: Fine-tuned SimCLR Model\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax4.scatter(\n",
    "            umap_simclr_model_finetuned_features[mask, 0], \n",
    "            umap_simclr_model_finetuned_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax4.set_title('Fine-tuned SimCLR Model', fontsize=16)\n",
    "    ax4.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax4.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Plot 5: Fine-tuned Rotation Model\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax5.scatter(\n",
    "            umap_rotation_model_model_finetuned_features[mask, 0], \n",
    "            umap_rotation_model_model_finetuned_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax5.set_title('Fine-tuned Rotation Model', fontsize=16)\n",
    "    ax5.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax5.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Plot 6: PCA\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax6.scatter(\n",
    "            umap_pca_features[mask, 0], \n",
    "            umap_pca_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax6.set_title('PCA', fontsize=16)\n",
    "    ax6.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax6.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Create a horizontal colorbar at the bottom\n",
    "    cbar_ax = fig.add_subplot(gs[2, :])\n",
    "    norm = Normalize(vmin=0, vmax=num_classes-1)\n",
    "    cb = ColorbarBase(\n",
    "        cbar_ax, \n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        orientation='horizontal'\n",
    "    )\n",
    "    \n",
    "    # Set colorbar title\n",
    "    cb.set_label(f'Classes (Total: {num_classes})', fontsize=14)\n",
    "    \n",
    "    # Add ticks for each class on the colorbar\n",
    "    # For a large number of classes, we'll space them out\n",
    "    if num_classes <= 20:\n",
    "        # For a smaller number of classes, show all ticks\n",
    "        tick_positions = np.arange(num_classes)\n",
    "        tick_labels = [str(label) for label in unique_labels]\n",
    "    else:\n",
    "        # For many classes, show every nth class\n",
    "        n = max(1, num_classes // 20)  # Show at most 20 ticks\n",
    "        tick_positions = np.arange(0, num_classes, n)\n",
    "        tick_labels = [str(unique_labels[i]) for i in range(0, num_classes, n)]\n",
    "    \n",
    "    cb.set_ticks(tick_positions)\n",
    "    cb.set_ticklabels(tick_labels)\n",
    "    \n",
    "    # Calculate frequency of each class\n",
    "    class_counts = {}\n",
    "    for label in labels:\n",
    "        if label in class_counts:\n",
    "            class_counts[label] += 1\n",
    "        else:\n",
    "            class_counts[label] = 1\n",
    "    \n",
    "    # Add frequency information above the colorbar\n",
    "    if num_classes <= 40:\n",
    "        # Add text annotations for class frequencies\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            if i % (max(1, num_classes // 20)) == 0:  # Only show labels for ticks we're displaying\n",
    "                freq = class_counts.get(label, 0)\n",
    "                cbar_ax.text(\n",
    "                    i/(num_classes-1), \n",
    "                    1.1, \n",
    "                    f\"{freq}\", \n",
    "                    horizontalalignment='center',\n",
    "                    fontsize=8\n",
    "                )\n",
    "    \n",
    "    # Set the main title\n",
    "    plt.suptitle(f'Comparing Feature Representations Using UMAP Visualization\\n(Total Classes: {num_classes})', fontsize=20)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model: torch.nn.Module, batch_x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract features from the model.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to use for feature extraction.\n",
    "        batch_x (torch.Tensor): The input batch of images.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The extracted features.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        features.append(input[0].detach().cpu().numpy())\n",
    "    \n",
    "    if hasattr(model, \"fc\"):\n",
    "        hook = model.fc.register_forward_hook(hook_fn)\n",
    "    else:\n",
    "        hook = model[1].register_forward_hook(hook_fn)\n",
    "    model(batch_x)\n",
    "    hook.remove()\n",
    "    features = features[0]\n",
    "    return features.reshape(features.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part XIV. Evaluating the models on GTSRB dataset\n",
    "\n",
    "In the previous sections, we fine-tuned our models and performed linear probing to evaluate them on the classification dataset. While these are the preferred methods to evaluate SSL models, we can also use more lightweight evaluation techniques that don't require additional training.\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Extract features from all the models we've trained (including the fine-tuned versions) on the GTSRB dataset\n",
    "2. Visualize these features using UMAP and t-SNE with class labels to see how well-separated the different traffic sign classes are in the feature space\n",
    "3. Train KNN (K-Nearest Neighbors) classifiers on the extracted features\n",
    "\n",
    "The KNN approach is particularly interesting because it directly evaluates the quality of the feature space without requiring any additional parametric model training. If similar classes are clustered together in the feature space, a simple KNN classifier should perform well. This gives us another perspective on how well our different self-supervised learning approaches have structured the feature space.\n",
    "\n",
    "By comparing the KNN classification results across different models, we can gain additional insights into which approach produces the most discriminative features for our road sign classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr_model.eval()\n",
    "rotation_model.eval()\n",
    "scratch_model.eval()\n",
    "simclr_model_finetuned.eval()\n",
    "rotation_model_model_finetuned.eval()\n",
    "simclr_model = simclr_model.to(device)\n",
    "\n",
    "simclr_features = []\n",
    "rotation_features = []\n",
    "scratch_features = []\n",
    "simclr_model_finetuned_features = []\n",
    "rotation_model_model_finetuned_features = []\n",
    "pca_features = []\n",
    "labels_full = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, labels in tqdm(dl_eval, desc=\"Extracting Features\"):\n",
    "        batch_x = batch_x.to(device)\n",
    "        labels_full.extend(label for label in labels)\n",
    "        \n",
    "        # Extract features\n",
    "        simclr_features.append(simclr_model(batch_x).flatten(start_dim=1).cpu().numpy())\n",
    "        rotation_features.append(extract_features(rotation_model, batch_x))\n",
    "        scratch_features.append(extract_features(scratch_model, batch_x))\n",
    "        simclr_model_finetuned_features.append(extract_features(simclr_model_finetuned, batch_x))\n",
    "        rotation_model_model_finetuned_features.append(extract_features(rotation_model_model_finetuned, batch_x))\n",
    "        pca_features.append(pca.transform(batch_x.cpu().numpy().reshape(batch_x.shape[0], -1)))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "simclr_features = np.concatenate(simclr_features, axis=0)\n",
    "rotation_features = np.concatenate(rotation_features, axis=0)\n",
    "scratch_features = np.concatenate(scratch_features, axis=0)\n",
    "simclr_model_finetuned_features = np.concatenate(simclr_model_finetuned_features, axis=0)\n",
    "rotation_model_model_finetuned_features = np.concatenate(rotation_model_model_finetuned_features, axis=0)\n",
    "pca_features = np.concatenate(pca_features, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize our features with T-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(\n",
    "    simclr_features, \n",
    "    rotation_features, \n",
    "    scratch_features, \n",
    "    simclr_model_finetuned_features, \n",
    "    rotation_model_model_finetuned_features, \n",
    "    pca_features, \n",
    "    labels_full\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize our features with UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap(\n",
    "    simclr_features, \n",
    "    rotation_features, \n",
    "    scratch_features, \n",
    "    simclr_model_finetuned_features, \n",
    "    rotation_model_model_finetuned_features, \n",
    "    pca_features, \n",
    "    labels_full\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate our models using a KNN classifier. The K-Nearest Neighbors (KNN) classifier is a simple yet effective method for classification that works by finding the k nearest neighbors of a data point in the feature space and assigning the class label based on the majority class of those neighbors.\n",
    "\n",
    "Your task is to implement this evaluation using the `KNeighborsClassifier` from the `sklearn` library. For each set of features extracted from our different models, you should:\n",
    "\n",
    "1. Create a KNN classifier with k=5 neighbors\n",
    "2. Train the KNN classifier on the extracted features and the corresponding labels\n",
    "3. Evaluate the KNN classifier on the same training features (we're primarily interested in how well-structured the feature space is)\n",
    "4. Print the accuracy of the KNN classifier\n",
    "\n",
    "You'll need to repeat this process 6 times - once for each feature set we extracted (Scratch, SimCLR, Rotation, SimCLR fine-tuned, Rotation fine-tuned, and PCA).\n",
    "\n",
    "This evaluation will give us another quantitative measure of how well our different approaches have structured the feature space. Higher KNN accuracy indicates features that better separate the different classes, even with this simple non-parametric classifier.\n",
    "\n",
    "Note: While we're evaluating on the training set here (which isn't standard practice), the purpose is to assess the quality of the learned representations rather than generalization performance. The goal is to see which approach creates more discriminative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KNN Classifier\n",
    "print(f\"KNN Accuracy on Simclr Model Features: {knn_accuracy:.2f}\")\n",
    "\n",
    "\n",
    "### KNN Classifier\n",
    "print(f\"KNN Accuracy on Rotation Model Features: {knn_accuracy:.2f}\")\n",
    "\n",
    "### KNN Classifier\n",
    "print(f\"KNN Accuracy on Scratch Model Features: {knn_accuracy:.2f}\")\n",
    "\n",
    "### KNN Classifier\n",
    "print(f\"KNN Accuracy on Fine-tuned Simclr Model Features: {knn_accuracy:.2f}\")\n",
    "\n",
    "### KNN Classifier\n",
    "print(f\"KNN Accuracy on Fine-tuned Rotation Model Features: {knn_accuracy:.2f}\")\n",
    "\n",
    "### KNN Classifier\n",
    "print(f\"KNN Accuracy on PCA Features: {knn_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part XV. Evaluate the models on CIFAR-10 dataset\n",
    "\n",
    "In this part, we will evaluate our models on the CIFAR-10 dataset, which represents a transfer learning scenario to a completely different domain of images. The CIFAR-10 dataset is a well-known benchmark for image classification tasks. It consists of 60,000 images in 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck), with 6,000 images per class. The images are in color but are much smaller at 32x32 pixels in size, compared to our road sign images.\n",
    "\n",
    "By evaluating our models on this different dataset, we can assess how well the representations learned from road signs transfer to a more general image classification task. This cross-domain evaluation is particularly important for understanding the generalization capabilities of self-supervised learning approaches.\n",
    "\n",
    "We will use the same approach as in the previous part to evaluate the models on the CIFAR-10 dataset, focusing only on the test set to keep the evaluation simple and efficient. This will give us insights into which self-supervised learning approach produces more transferable features across different image domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.datasets.CIFAR10(root=path_where_data_is_stored, train=False, transform=test_transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part XVI. Evaluate the models on Pet dataset\n",
    "\n",
    "The last part of our exercise is to evaluate the models on the Oxford-IIIT Pet dataset. This dataset presents another domain transfer challenge, focusing on fine-grained classification of pet breeds. It consists of 37 categories of pet images (different breeds of cats and dogs), with approximately 200 images per class for a total of around 7,400 images. The images are in color and will be resized to 224x224 pixels in our processing pipeline.\n",
    "\n",
    "The Pet dataset is particularly interesting for our evaluation because:\n",
    "1. It represents a fine-grained classification task (distinguishing between similar-looking breeds)\n",
    "2. It contains natural images with varying backgrounds, poses, and lighting conditions\n",
    "3. It's different from both road signs (our training domain) and CIFAR-10 objects\n",
    "\n",
    "By evaluating our models across these three distinct datasets (GTSRB road signs, CIFAR-10 objects, and Oxford-IIIT Pet breeds), we can comprehensively assess which self-supervised learning approach produces more robust and transferable visual representations.\n",
    "\n",
    "We will use the same approach as in the previous parts to evaluate the models on the Pet dataset, focusing only on the test set. This final evaluation will complete our understanding of the transfer learning capabilities of our different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.datasets.OxfordIIITPet(root=path_where_data_is_stored, split='test', target_types='category', transform=test_transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REPORT\n",
    "\n",
    "Now the main part of our exercise is the report. I want you to write a short report summarizing your findings. The report should include:\n",
    "1. A brief introduction to the task and the models you trained\n",
    "2. A description of the datasets used for training and evaluation\n",
    "3. A summary of the training process, including results and any challenges faced\n",
    "4. A comparison of the performance of the different downstream task evaluations (linear probing, fine-tuning, KNN) on the GTSRB dataset\n",
    "5. What conclusions can be drawn from the visualizations of the features using UMAP and t-SNE?\n",
    "6. A comparison of the performance of the different models (PCA vs SSL vs. scratch) on the GTSRB dataset\n",
    "7. A similar comparison of the performance of the different models on the CIFAR-10 and Pet datasets\n",
    "8. A discussion of the transferability of the learned representations across different datasets\n",
    "9. Any additional insights or observations you made during the exercise\n",
    "\n",
    "Additionally I want you to answer the following questions:\n",
    "1. What are the main differences between linear probing and fine-tuning in the context of self-supervised learning?\n",
    "2. How does the choice of pretext task (rotation vs. contrastive learning) affect the quality of learned representations?\n",
    "3. What are the advantages and disadvantages of using PCA for dimensionality reduction compared to deep learning-based methods like SimCLR?\n",
    "4. We use normalization and standardization in our preprocessing stage using predefined values. Where do these values come from?\n",
    "\n",
    "You can send me the notebook with the markdown cell with the report via Slack. Now just send me the notebook; I don't need your Google Colab link. I will check the notebook and send you feedback. If you have any questions or problems with the notebook, feel free to ask me on Slack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
