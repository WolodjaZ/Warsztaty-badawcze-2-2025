{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepared by Vladimir Zaigrajew and Tymoteusz KwieciÅ„ski\n",
    "\n",
    "# Exercise Week 4 - Evaluation of Self-supervised Learning Methods\n",
    "In the previous exercise we learned how to train self-supervised models on the ration task on the dataset with traffic signs. We later used the trained models to extract features and train a classifier on top of them. We compared the performance of the classifier trained on the features extracted from the self-supervised model, fine-tuned self-supervised model on downstream task (classification), and the classifier trained from scratch on the same dataset. In this exercise we will focus on evaluation part of training self-supervised models. In previous exercise you done the downstream task evaluation on the dataset with traffic signs. In this exercise we will focus on less compute intensive evaluation using visualization techniques and alternative to learning linear probing (linear classifier) from previous exercise.\n",
    "\n",
    "In this exercise, we will focus on **4** different models:\n",
    "- ResNet-18 trained with SSL task (Rotation Prediction - same as in the previous exercise)\n",
    "- ResNet-18 trained from the scratch (Supervised)\n",
    "- ResNet-18 pertained from Pytorch team on ImageNet (Supervised)\n",
    "- PCA model (classical unsupervised learning method)\n",
    "\n",
    "\n",
    "For training a few of our models we will use STL10 dataset for self-supervised training and CIFAR-100 dataset for supervised training. The STL10 dataset consists of 10 classes with 105000 images in set where some of the images are not labeled (don't care because we will be using it for self-supervised training). The dataset have also the labeled test set which consist of 8000 images. The CIFAR-100 dataset consists of 60,000 32x32 color images in 100 classes, with 600 images per class. The dataset is divided into 50,000 training images and 10,000 test images.\n",
    "\n",
    "After we train our models we will evaluate learned representations on following datasets:\n",
    "- CIFAR-10: Subset of CIFAR-100 with 10 classes (6000 images in total)\n",
    "- GTSRB: Dataset with 43 classes of traffic signs (600 images per class) which you may already know from the previous exercise\n",
    "- FGVCAircraft: Dataset with 100 classes of aircrafts (100 images per class)\n",
    "\n",
    "This exercise will be less compute intensive as your task will be to analyse the results from the evaluation of the models we have on various task and writing a short report on the results.\n",
    "\n",
    "So let's start!\n",
    "\n",
    "Part I. Prepare the environment\n",
    "Firstly, prepare an environment by installing all the required libraries. To know which package to install you need to investigate cells with the import statements. I require to paste the code below with the command to install the libraries with **specific versions** for example:\n",
    "```python\n",
    "%pip install numpy==1.21.0 pandas==1.3.0 matplotlib==3.4.2 torch==1.9.0 torchvision==0.10.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a placeholder for package installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the package is installed, you can run the following command to check if it works as packages imported in the cell below are needed for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "from matplotlib.colors import Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally in previous exercise I asked you about how results can be reproducible specifically in the context seeding the random number generator. In the code below we set all the random number generators to the same seed. This is important for reproducibility of the results. You can change the seed to any number you want, but make sure to set it to the same number in all cells where you need to set the random number generator. People who still don't know how it works should read the blogs about it as this is basic knowledge for ML practitioners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed everything for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed: int=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Prepare the data\n",
    "\n",
    "Now, we need to prepare the data. We need to get the CIFAR-100 and STL10 dataset for training and the other datasets for evaluation. We will use the torchvision library to download and prepare the datasets. I recommend to check each of the dataset how they are structured, the domain of the data, the classes and the sizes of the data as this may help you when you will be analyzing the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the transform to apply to the data\n",
    "# the training transform includes random cropping and resizing. Random cropping is used to\n",
    "# augment the data and make the model more robust to variations in the input data.\n",
    "# The size 128 was chosen to be the most compatible with all datasets (it is also good to consider\n",
    "# the size of the original images when writing the report).\n",
    "transform_train = T.Compose([\n",
    "    T.RandomResizedCrop(128),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "# The test transform includes resizing and center cropping to ensure that the input data is\n",
    "# the same size as the training data (128x128).\n",
    "test_transform = T.Compose([\n",
    "    T.Resize(134),\n",
    "    T.CenterCrop(128),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset_path = './data'\n",
    "\n",
    "# load the STL10 dataset\n",
    "train_dataset_ssl = torchvision.datasets.STL10(root=dataset_path, split='train+unlabeled', download=True, transform=transform_train)\n",
    "test_dataset_ssl = torchvision.datasets.STL10(root=dataset_path, split='test', download=True, transform=test_transform)\n",
    "\n",
    "# load the CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR100(root=dataset_path, train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR100(root=dataset_path, train=False, download=True, transform=transform_train)\n",
    "\n",
    "# load the CIFAR-10 test dataset\n",
    "test_dataset_cifar10 = torchvision.datasets.CIFAR10(root=dataset_path, train=False, download=True, transform=test_transform)\n",
    "\n",
    "# load the GTSRB test dataset\n",
    "test_dataset_gtsrb = torchvision.datasets.GTSRB(root=dataset_path, download=True, transform=test_transform)\n",
    "\n",
    "# load the FGVCAircraft test dataset\n",
    "test_dataset_fgv_aircraft = torchvision.datasets.FGVCAircraft(root=dataset_path, split='test', download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the dataset class for rotation prediction task based on STL10 dataset. In previous exercise you implemented the dataset class for the rotation prediction task. Now I have already done this so you can use the code below.\n",
    "\n",
    "The dataset class is based on the STL10 dataset and it will create a dataset with images rotated by 0, 90, 180, and 270 degrees. The labels will be the rotation angle (0, 1, 2, or 3). The dataset class will also apply the same transformations as in the previous exercise. You can check the code below to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation Dataset\n",
    "class SSLRot(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset: torch.utils.data.Dataset, angles: list[int]):\n",
    "        \"\"\"\n",
    "        Initialize the rotation dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset (torch.utils.data.Dataset): The original dataset to apply rotations to.\n",
    "            angles (list[int]): List of rotation angles in degrees (e.g. [0, 90, 180, 270]).\n",
    "        \"\"\"\n",
    "        super(SSLRot, self).__init__()\n",
    "        self.original_dataset = dataset\n",
    "        self.angles = angles\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the length of the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            int: The number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.original_dataset)\n",
    "    \n",
    "    def rand_rotate(self, img: torch.Tensor) -> tuple[torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Randomly rotates the image by 0, 90, 180, or 270 degrees.\n",
    "\n",
    "        Args:\n",
    "            img (torch.Tensor): Input image tensor of shape (C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            tuple: Rotated image tensor and the corresponding rotation label (0, 1, 2, or 3).\n",
    "        \"\"\"\n",
    "        rot_label = random.randint(0, 3)\n",
    "        rotated_img = T.functional.rotate(img, self.angles[rot_label])\n",
    "        return rotated_img, rot_label\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieves the rotated image and its corresponding rotation label.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Rotated image tensor and the rotation label as a tensor.\n",
    "        \"\"\"\n",
    "        img, _ = self.original_dataset[idx]\n",
    "        rotated_img, rot_label = self.rand_rotate(img)\n",
    "        return rotated_img, torch.tensor(rot_label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rotation angles\n",
    "angles = [0, 90, 180, 270] \n",
    "\n",
    "# Create the rotation datasets for training and testing\n",
    "rotation_dataset_train = SSLRot(train_dataset_ssl, angles)\n",
    "rotation_dataset_test = SSLRot(test_dataset_ssl, angles)\n",
    "\n",
    "# Print the lengths of the datasets\n",
    "len(rotation_dataset_train), len(rotation_dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Load the models\n",
    "\n",
    "Now, we need to get the model. In the previous exercise we used the `ResNet18` model from `torchvision`. This time we will use the same model. Your task is to load the models and modify them if needed. As described earlier we will use the following models:\n",
    "- ResNet-18 trained with SSL task - so we need to load the empty ResNet-18 model (no weights) and modify the `fc` layer to have 4 outputs (for the rotation prediction task)\n",
    "- ResNet-18 trained from scratch - so we need to load the empty ResNet-18 model (no weights) and modify the `fc` layer to have 10 outputs (for the CIFAR-10 classification task)\n",
    "- ResNet-18 pretrained from Pytorch team on `IMAGENET1K_V1` - so we need to load the ResNet-18 model with the weights from ImageNet and **modify nothing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a placeholder for model loading please name the models:\n",
    "# ssl_model\n",
    "# scratch_model\n",
    "# model_finetuned (the pretrained one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV. Prepare the training\n",
    "\n",
    "In the part below you don't need to do anything as the code is already prepared for you. The code below will execute the necessary training and validation steps needed to train both SSL and from scratch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model: torch.nn.Module, optimizer: torch.optim.Optimizer, train_loader: torch.utils.data.DataLoader, criterion: torch.nn.Module, device=torch.device) -> tuple[float, float]:\n",
    "    \"\"\"Train the model for one epoch.\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to use.\n",
    "        train_loader (torch.utils.data.DataLoader): The training data loader.\n",
    "        criterion (torch.nn.Module): The loss function.\n",
    "        device (torch.device): The device to use for training (CPU or GPU).\n",
    "    Returns:\n",
    "        tuple: The average loss and accuracy for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    train_loss = total_loss / total\n",
    "    train_acc = 100.0 * correct / total\n",
    "    \n",
    "    return train_loss, train_acc\n",
    "\n",
    "def validate(model: torch.nn.Module, val_loader: torch.utils.data.DataLoader, device=torch.device) -> float:\n",
    "    \"\"\"Validate the model.\n",
    "    Args:\n",
    "        model (nn.Module): The model to validate.\n",
    "        val_loader (torch.utils.data.DataLoader): The validation data loader.\n",
    "        device (torch.device): The device to use for validation (CPU or GPU).\n",
    "    \n",
    "    Returns:\n",
    "        float: The average accuracy for the validation set.    \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader, desc=\"Validating\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Statistics\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    val_acc = 100.0 * correct / total\n",
    "    \n",
    "    return val_acc\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module, train_loader: torch.utils.data.DataLoader, val_loader: torch.utils.data.DataLoader, optimizer: torch.optim.Optimizer, criterion: torch.nn.Module, num_epochs: int=10, device=torch.device) -> tuple[list[float], list[float], list[dict]]:\n",
    "    \"\"\"Train the model.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train.\n",
    "        train_loader (torch.utils.data.DataLoader): The training data loader.\n",
    "        val_loader (torch.utils.data.DataLoader): The validation data loader.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to use.\n",
    "        criterion (torch.nn.Module): The loss function.\n",
    "        num_epochs (int): The number of epochs to train for.\n",
    "        device (torch.device): The device to use for training (CPU or GPU).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the training accuracy, validation accuracy, and model state dictionaries.\n",
    "    \"\"\"\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    state_dicts = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(model, optimizer, train_loader, criterion, device)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Save the model state\n",
    "        state_dicts.append(model.state_dict())\n",
    "        \n",
    "        # Validate\n",
    "        val_acc = validate(model, val_loader, device)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "    return train_accs, val_accs, state_dicts\n",
    "\n",
    "\n",
    "def visualize_predictions(model: torch.nn.Module, dataset: torch.utils.data.Dataset, device: torch.device, class_names: list[str], num_images: int=5):\n",
    "    \"\"\"Visualize predictions of the model on a subset of the dataset.\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to use for predictions.\n",
    "        dataset (torch.utils.data.Dataset): The dataset to visualize.\n",
    "        device (torch.device): The device to use for predictions (CPU or GPU).\n",
    "        class_names (list[str]): The list of class names.\n",
    "        num_images (int): The number of images to visualize.\n",
    "        \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    indices = random.sample(range(len(dataset)), num_images)\n",
    "    \n",
    "    _, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            img, label = dataset[idx]\n",
    "            img = img.unsqueeze(0).to(device)\n",
    "            output = model(img)\n",
    "            pred_label = output.argmax(dim=1).item()\n",
    "            \n",
    "            axes[i].imshow(img.squeeze(0).permute(1, 2, 0).cpu())\n",
    "            axes[i].set_title(f\"Pred: {class_names[pred_label]}\\nTrue: {class_names[label]}\")\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V. Train SSL and Supervised models\n",
    "\n",
    "In this part we will train and evaluate both models using the rotation dataset (STL10) and CIFAR-10 datasets. As I promised earlier, this part can be resource intensive, so I already done the training and on our slack group you can find the weights for trained models. You **can** train the model by yourself, but you can also just load the weights I provided. I will not tell you which cell is loading the weights you need to figure it out based on the code below :)\n",
    "\n",
    "In the evaluation phase of our training we will plot the accuracy of the model on the train and validation datasets. We will also plot example images with their predicted labels to visually assess the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS ###\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_WORKERS = 0\n",
    "NUMBER_OF_CLASSES = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data loaders\n",
    "train_dl = torch.utils.data.DataLoader(rotation_dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_dl = torch.utils.data.DataLoader(rotation_dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Prepare the model\n",
    "ssl_model = ssl_model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.AdamW(ssl_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train_accs, val_accs, state_dicts = train(ssl_model, train_dl, val_dl, optimizer, criterion, num_epochs=NUM_EPOCHS, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accs, label='train accuracy')\n",
    "plt.plot(val_accs, label='val accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy on Rotation Dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "arg_max = np.argmax(val_accs)\n",
    "print(f\"Best model at epoch {arg_max+1} with accuracy {val_accs[arg_max]:.2f}%\")\n",
    "ssl_model.load_state_dict(state_dicts[arg_max])\n",
    "torch.save(ssl_model.state_dict(), os.path.join(dataset_path, 'ssl_model.pth'))\n",
    "\n",
    "# Load the model\n",
    "# ssl_model.load_state_dict(torch.load(os.path.join(dataset_path, 'ssl_model.pth')))\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Check the performance of the model on the test set\n",
    "# train_dl = torch.utils.data.DataLoader(rotation_dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "# val_dl = torch.utils.data.DataLoader(rotation_dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# train_acc = validate(ssl_model, train_dl, device)\n",
    "# print(f\"Train Accuracy: {train_acc:.2f}%\")\n",
    "# val_acc = validate(ssl_model, val_dl, device)\n",
    "# print(f\"Validation Accuracy: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(ssl_model, rotation_dataset_test, num_images=5, device=device, class_names=angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay we trained our SSL model let's now train our supervised model on CIFAR-10. The drill is the same as with the previous model you can either train the model by yourself or load the weights I provided. The code below is already prepared for you so you don't need to do anything. You can just run the code and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS ###\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_WORKERS = 0\n",
    "NUMBER_OF_CLASSES = 43\n",
    "device = torch.device(\"mps\")    \n",
    "print(f\"Using device: {device}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_dl = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Load the model\n",
    "scratch_model = scratch_model.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.AdamW(scratch_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train_accs, val_accs, state_dict = train(scratch_model, train_dl, val_dl, optimizer, criterion, num_epochs=NUM_EPOCHS, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accs, label='train accuracy')\n",
    "plt.plot(val_accs, label='val accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy on CIFAR-10 Dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "arg_max = np.argmax(val_accs)\n",
    "print(f\"Best model at epoch {arg_max+1} with accuracy {val_accs[arg_max]:.2f}%\")\n",
    "scratch_model.load_state_dict(state_dict[arg_max])\n",
    "torch.save(scratch_model.state_dict(), os.path.join(dataset_path, 'scratch_model.pth'))\n",
    "\n",
    "# Load the model\n",
    "# scratch_model.load_state_dict(torch.load(os.path.join(dataset_path, 'scratch_model.pth')))\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Check the performance of the model on the test set\n",
    "# train_acc = validate(scratch_model, train_dl, device)\n",
    "# print(f\"Train Accuracy: {train_acc:.2f}%\")\n",
    "# val_acc = validate(scratch_model, val_dl, device)\n",
    "# print(f\"Validation Accuracy: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(scratch_model, test_dataset, num_images=5, device=device, class_names=train_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VI. Prepare the feature extraction from ResNet models and visualization methods\n",
    "\n",
    "The code below which is already prepared for you implement the feature extraction from the ResNet models. So using the function `extract_features` you can extract the features from the provided ResNet style model and the dataset.\n",
    "\n",
    "The second function `plot_tsne` is used to plot the t-SNE visualization of the features. The t-SNE is a technique for dimensionality reduction that is particularly well-suited for visualizing high-dimensional data. It works by converting the similarities between data points into joint probabilities and then minimizing the Kullback-Leibler divergence between the joint probabilities and the corresponding probabilities in the lower-dimensional space.\n",
    "\n",
    "We will provide to `plot_tsne` features extracted by the `extract_features` function from all our models. The function will convert features to 2D space and plot them using the `matplotlib` library. We also provide the labels for the data points to color them according to their class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model: torch.nn.Module, batch_x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract features from the model.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to use for feature extraction.\n",
    "        batch_x (torch.Tensor): The input batch of images.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The extracted features.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        features.append(input[0].detach().cpu().numpy())\n",
    "    \n",
    "    hook = model.fc.register_forward_hook(hook_fn)\n",
    "    model(batch_x)\n",
    "    hook.remove()\n",
    "    features = features[0]\n",
    "    return features.reshape(features.shape[0], -1)\n",
    "\n",
    "def plot_tsne(\n",
    "    scratch_features: np.ndarray,\n",
    "    ssl_features: np.ndarray,\n",
    "    model_finetuned_features: np.ndarray,\n",
    "    pca_features: np.ndarray,\n",
    "    labels: np.ndarray\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the features using t-SNE with a color gradient bar at the bottom.\n",
    "\n",
    "    Args:\n",
    "        scratch_features (np.ndarray): Features extracted from the model trained from scratch.\n",
    "        ssl_features (np.ndarray): Features extracted from the model trained with SSL.\n",
    "        model_finetuned_features (np.ndarray): Features extracted from the pretrained model by Pytorch team.\n",
    "        pca_features (np.ndarray): Features extracted from PCA.\n",
    "        labels (np.ndarray): Class labels for the features.\n",
    "    \"\"\"\n",
    "    # Get TSNE\n",
    "    scaler = StandardScaler()\n",
    "    scaler_scratch_features = scaler.fit_transform(scratch_features)\n",
    "    scaler_ssl_features = scaler.fit_transform(ssl_features)\n",
    "    scaler_model_finetuned_features = scaler.fit_transform(model_finetuned_features)\n",
    "    scaler_pca_features = scaler.fit_transform(pca_features)\n",
    "    \n",
    "    # Apply TSNE\n",
    "    tsne = TSNE(n_components=2, random_state=SEED)\n",
    "    tsne_scratch_features = tsne.fit_transform(scaler_scratch_features)\n",
    "    tsne_ssl_features = tsne.fit_transform(scaler_ssl_features)\n",
    "    tsne_model_finetuned_features = tsne.fit_transform(scaler_model_finetuned_features)\n",
    "    tsne_pca_features = tsne.fit_transform(scaler_pca_features)\n",
    "    \n",
    "    # Create a figure with GridSpec for better control\n",
    "    fig = plt.figure(figsize=(20, 18))\n",
    "    gs = gridspec.GridSpec(3, 2, height_ratios=[1, 1, 0.15])\n",
    "    \n",
    "    # Get unique classes for colormap\n",
    "    unique_labels = np.unique(labels)\n",
    "    num_classes = len(unique_labels)\n",
    "    \n",
    "    # Create a color map\n",
    "    cmap = plt.cm.get_cmap('viridis', num_classes)\n",
    "    colors = [cmap(i) for i in range(num_classes)]\n",
    "    \n",
    "    # Plot 1: Model Trained from Scratch\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax1.scatter(\n",
    "            tsne_scratch_features[mask, 0], \n",
    "            tsne_scratch_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7,\n",
    "            label=str(label)  # Convert label to string for plotting\n",
    "        )\n",
    "    ax1.set_title('Model Trained from Scratch', fontsize=16)\n",
    "    ax1.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax1.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Plot 2: Model Trained with SSL\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax2.scatter(\n",
    "            tsne_ssl_features[mask, 0], \n",
    "            tsne_ssl_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax2.set_title('Model Trained with SSL', fontsize=16)\n",
    "    ax2.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax2.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Plot 3: Fine-tuned SSL Model\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax3.scatter(\n",
    "            tsne_model_finetuned_features[mask, 0], \n",
    "            tsne_model_finetuned_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax3.set_title('Fine-tuned SSL Model', fontsize=16)\n",
    "    ax3.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax3.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Plot 4: PCA\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels == label\n",
    "        ax4.scatter(\n",
    "            tsne_pca_features[mask, 0], \n",
    "            tsne_pca_features[mask, 1],\n",
    "            color=colors[i], \n",
    "            s=20, \n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax4.set_title('PCA', fontsize=16)\n",
    "    ax4.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    ax4.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    \n",
    "    # Create a horizontal colorbar at the bottom\n",
    "    cbar_ax = fig.add_subplot(gs[2, :])\n",
    "    norm = Normalize(vmin=0, vmax=num_classes-1)\n",
    "    cb = ColorbarBase(\n",
    "        cbar_ax, \n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        orientation='horizontal'\n",
    "    )\n",
    "    \n",
    "    cb.set_label(f'Classes (Total: {num_classes})', fontsize=14)\n",
    "    \n",
    "    # Add ticks for each class on the colorbar\n",
    "    if num_classes <= 20:\n",
    "        # For a smaller number of classes, show all ticks\n",
    "        tick_positions = np.arange(num_classes)\n",
    "        tick_labels = [str(label) for label in unique_labels]\n",
    "    else:\n",
    "        # For many classes, show every nth class\n",
    "        n = max(1, num_classes // 20)\n",
    "        tick_positions = np.arange(0, num_classes, n)\n",
    "        tick_labels = [str(unique_labels[i]) for i in range(0, num_classes, n)]\n",
    "    \n",
    "    cb.set_ticks(tick_positions)\n",
    "    cb.set_ticklabels(tick_labels)\n",
    "    \n",
    "    # Calculate frequency of each class\n",
    "    class_counts = {}\n",
    "    for label in labels:\n",
    "        if label in class_counts:\n",
    "            class_counts[label] += 1\n",
    "        else:\n",
    "            class_counts[label] = 1\n",
    "    \n",
    "    # Add frequency information above the colorbar\n",
    "    if num_classes <= 40:\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            if i % (max(1, num_classes // 20)) == 0:\n",
    "                freq = class_counts.get(label, 0)\n",
    "                cbar_ax.text(\n",
    "                    i/(num_classes-1), \n",
    "                    1.1, \n",
    "                    f\"{freq}\", \n",
    "                    horizontalalignment='center',\n",
    "                    fontsize=8\n",
    "                )\n",
    "    \n",
    "    # Set the main title\n",
    "    plt.suptitle(f'Comparing Feature Representations Using t-SNE Visualization\\n(Total Classes: {num_classes})', fontsize=20)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the lecture I mentioned that there is a newer method for visualization called UMAP. Your task is to similar how it is done in `plot_tsne` function to implement the UMAP visualization. You can use the `umap-learn` library for this task. You can find the documentation here: https://umap-learn.readthedocs.io/en/latest/. The UMAP is a more recent method for dimensionality reduction that is often faster and produces better results than t-SNE. It is based on the concept of topological data analysis and uses a different approach to minimize the divergence between the high-dimensional and low-dimensional representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap(\n",
    "    scratch_features: np.ndarray,\n",
    "    ssl_features: np.ndarray,\n",
    "    model_finetuned_features: np.ndarray,\n",
    "    pca_features: np.ndarray,\n",
    "    labels: np.ndarray\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualize the features using UMAP. This function first takes the features provided to the function and applies\n",
    "    UMAP to reduce the dimensionality of the features to 2D. It then creates a 2x2 grid of subplots, where each\n",
    "    subplot corresponds to a different set of features. The first subplot shows the features extracted from the model\n",
    "    trained from scratch, the second subplot shows the features extracted from the model trained with SSL, the third\n",
    "    subplot shows the features extracted from the fine-tuned SSL model, and the fourth subplot shows the PCA features.\n",
    "    Each subplot contains a scatter plot where each point is colored according to its class label. The legend is\n",
    "    shown in the fourth subplot.\n",
    "    \n",
    "    Args:\n",
    "        scratch_features (np.ndarray): Features extracted from the model trained from scratch.\n",
    "        ssl_features (np.ndarray): Features extracted from the model trained with SSL.\n",
    "        model_finetuned_features (np.ndarray): Features extracted from the pretrained model by Pytorch team.\n",
    "        pca_features (np.ndarray): Features extracted from PCA.\n",
    "        labels (np.ndarray): Class labels for the features.\n",
    "    \"\"\"\n",
    "    # CODE FOR UMAP VISUALIZATION\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VII. Train the PCA model\n",
    "\n",
    "In this part we will train the PCA model. The PCA is a classical unsupervised learning method that is used for dimensionality reduction. I will not get into the mathematical details of PCA as you should already know how it works from the previous courses. In the cell below you need to train the PCA code on the CIFAR-10 dataset (but if you downloaded STL10 dataset I recommend using that instead, but the test set). The thing I require is to have PCA working with **flattened images** and the number of components set to the same as in dimension in embeddings we will be using from Resnet models `scratch_model.fc.in_features`. If PCA trains too long you can use the subset of the dataset (I used random sample of 5000 images). You also need to ensure that the PCA is standardized before fitting (Use `Pipeline` from `sklearn` library to combine the transformation steps wit PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for the PCA model you need to create and train\n",
    "\n",
    "# Visualize the explained variance ratio of the PCA components\n",
    "plt.plot(np.cumsum(pca.named_steps['pca'].explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VIII. Evaluate the models on CIFAR-10 dataset\n",
    "\n",
    "In this part we will evaluate the models on the CIFAR-10 dataset. The CIFAR-10 dataset is a subset of CIFAR-100 dataset with lesser number of classes. In this task we will extract features from our 4 models and try to evaluate them based on our dataset. \n",
    "\n",
    "Our evaluation will include the visualization of the features using t-SNE and UMAP methods. For performance evaluation as an alternative to linear probing we will use the KNN classifier. The KNN classifier is a simple and effective method for classification that works by finding the k nearest neighbors of a data point in the feature space and assigning the class label based.\n",
    "\n",
    "The cell below will extract features from our models using the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Move models to eval state and to the same device\n",
    "scratch_model.eval()\n",
    "ssl_model.eval()\n",
    "model_finetuned.eval()\n",
    "\n",
    "model_finetuned = model_finetuned.to(device)\n",
    "\n",
    "# Lists to store features\n",
    "scratch_cifar10 = []\n",
    "ssl_cifar10 = []\n",
    "model_finetuned_cifar10 = []\n",
    "pca_cifar10 = []\n",
    "cifar10_labels = []\n",
    "\n",
    "# Create a DataLoader for the test dataset (it is more efficient to use a DataLoader)\n",
    "dl = torch.utils.data.DataLoader(test_dataset_cifar10, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "for batch_x, labels in tqdm(dl, desc=\"Extracting features from CIFAR-10 test set\"):\n",
    "    batch_x = batch_x.to(device)\n",
    "    cifar10_labels.extend(label for label in labels)\n",
    "    \n",
    "    # Extract features\n",
    "    scratch_features = extract_features(scratch_model, batch_x)\n",
    "    ssl_features = extract_features(ssl_model, batch_x)\n",
    "    model_finetuned_features = extract_features(model_finetuned, batch_x)\n",
    "    pca_features = pca.transform(batch_x.cpu().numpy().reshape(batch_x.shape[0], -1))\n",
    "    \n",
    "    # Fill the lists\n",
    "    scratch_cifar10.append(scratch_features)\n",
    "    ssl_cifar10.append(ssl_features)\n",
    "    model_finetuned_cifar10.append(model_finetuned_features)\n",
    "    pca_cifar10.append(pca_features)\n",
    "\n",
    "# Concatenate the features from all batches\n",
    "scratch_cifar10 = np.concatenate(scratch_cifar10, axis=0)\n",
    "ssl_cifar10 = np.concatenate(ssl_cifar10, axis=0)\n",
    "model_finetuned_cifar10 = np.concatenate(model_finetuned_cifar10, axis=0)\n",
    "pca_cifar10 = np.concatenate(pca_cifar10, axis=0)\n",
    "scratch_cifar10.shape, ssl_cifar10.shape, model_finetuned_cifar10.shape, pca_cifar10.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize our features with TSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(\n",
    "    scratch_cifar10, \n",
    "    ssl_cifar10, \n",
    "    model_finetuned_cifar10, \n",
    "    pca_cifar10, \n",
    "    cifar10_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize our features with UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap(\n",
    "    scratch_cifar10,\n",
    "    ssl_cifar10,\n",
    "    model_finetuned_cifar10, \n",
    "    pca_cifar10, \n",
    "    cifar10_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate our models using KNN classifier. The KNN classifier is a simple and effective method for classification that works by finding the k nearest neighbors of a data point in the feature space and assigning the class label based on the majority class of those neighbors. Your task is to implement this evaluation using the `KNeighborsClassifier` from the `sklearn` library. Each features extracted from the model should be used as input to the separate KNN classifier. The KNN classifier should be trained on the extracted features and labels used in the visualization. For the number of neighbors you can use 5. For the evaluation part of KNN you can use build in function `score` from the `KNeighborsClassifier` class and print the accuracy of the model on the same training features. So shortly you need to implement the following steps:\n",
    "1. Create the KNN classifier with 5 neighbors\n",
    "2. Train the KNN classifier on the extracted features and labels\n",
    "3. Evaluate the KNN classifier on the same training features\n",
    "4. Print the accuracy of the KNN classifier\n",
    "\n",
    "And do it 4 times for each feature we extracted from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for the KNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IX. Evaluate the models on GTSRB dataset\n",
    "In this part we will evaluate the models on the GTSRB dataset. The GTSRB dataset is a dataset with 43 classes of traffic signs. The task is the same as in the previous part. We will extract features from our models and try to evaluate them with visualizations and KNN classifier. You need to write the whole code for this evaluation but as you may guess, you just need to copy the code from the previous section and change the datasets from CIFAR-10 to GTSRB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your time to shine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part X. Evaluate the models on FGVCAircraft dataset\n",
    "You know the drill, copy the code, change the dataset and run the code. The FGVCAircraft dataset is a dataset with 100 classes of aircrafts. The task is the same as in the previous part. We will extract features from our models and try to evaluate them with visualizations and KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And one last time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the main part of our exercise so the report.\n",
    "\n",
    "In this part you need to write a short report (really short don't overdo it) on the results of the evaluation. You can use the following questions to guide you in writing the report:\n",
    "- What can be read from the t-SNE and UMAP visualizations?\n",
    "- What are the advantages and disadvantages of using t-SNE and UMAP for visualization?\n",
    "- What are the advantages and disadvantages of using KNN classifier for evaluation?\n",
    "- What are the results of the evaluation on the CIFAR-100 dataset?\n",
    "- What are the results of the evaluation on the GTSRB dataset?\n",
    "- What are the results of the evaluation on the FGVCAircraft dataset?\n",
    "- Why the results are different for each dataset? (The hint is to think about the domain of the data and the domain that model was trained on and the task it solved)\n",
    "- Why do you think the best performing model for each dataset is better than the others? Answer this question for each dataset separately.\n",
    "\n",
    "**I want you to know that this notebook is not about showing you that SSL is awesome but to demonstrate how we can evaluate models features and how we can use them for different tasks. I want you to understand that showing only the results is often not enough and you need to know how to interpret them in the context of the current task and in future to improve the results. That's why I am more focused about your short report not the results of the notebook.**\n",
    "\n",
    "You can send me the notebook with the markdown cell with the report via slack. Now just send me the notebook I don't need you google colab link. I will check the notebook and send you the feedback. If you have any questions or problems with the notebook, feel free to ask me on slack.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
